{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIS08 / OR92 Data Modeling: Python - Web Scraping\n",
    "\n",
    "Timo Breuer, Faculty of Information Science and Communication Studies, Institute of Information Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** Some contents are taken from https://carpentries-incubator.github.io/lc-webscraping/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Basics of the Web, HTTP, and HTML\n",
    "---\n",
    "\n",
    "### What is the Web?\n",
    "\n",
    "- A collection of interconnected documents and resources accessed via the internet.\n",
    "- Web browsers (e.g., Chrome, Firefox, Safari) are used to navigate the web.\n",
    "\n",
    "### What is HTTP?\n",
    "\n",
    "**HTTP = HyperText Transfer Protocol:**\n",
    "- It is a protocol for transferring data between a client (browser) and a server.\n",
    "  \n",
    "**Key Features:**\n",
    "- Request-Response Model\n",
    "- Stateless (no memory of previous requests).\n",
    "  \n",
    "**HTTP Methods:**\n",
    "- `GET`: Retrieve data.\n",
    "- `POST`: Send data.\n",
    "- `PUT`: Update data.\n",
    "- `DELETE`: Remove data.\n",
    "\n",
    "Example Request:\n",
    "```\n",
    "GET /index.html HTTP/1.1\n",
    "Host: example.com\n",
    "```\n",
    "\n",
    "Example Response:\n",
    "```\n",
    "HTTP/1.1 200 OK\n",
    "Content-Type: text/html\n",
    "```\n",
    "\n",
    "### What is HTML?\n",
    "\n",
    "**HTML (HyperText Markup Language):**\n",
    "- Language for structuring web pages.\n",
    "- Defines content and layout.\n",
    "\n",
    "**HTML Elements:**\n",
    "- Exmaple tags: `<h1>`, `<p>`, `<a>`.\n",
    "- Example attributes: class, id, href.\n",
    "\n",
    "**Basic HTML Example:**\n",
    "```\n",
    "<html>\n",
    "  <head>\n",
    "    <title>My Web Page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Welcome to My Page</h1>\n",
    "    <p>This is a paragraph.</p>\n",
    "    <a href=\"https://example.com\">Click Here</a>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "\n",
    "### How Does a Web Page Load?\n",
    "\n",
    "**Step-by-Step Process:**\n",
    "1.\tBrowser sends an HTTP request to a server.\n",
    "2.\tServer processes the request and sends an HTTP response (usually HTML).\n",
    "3.\tBrowser renders the HTML, CSS, and JavaScript to display the page.\n",
    "\n",
    "[![](https://mermaid.ink/img/pako:eNp1UctuwjAQ_BXLJ5DSNDHkeeBQ-qBVKyGSU5WLG2-TCGKntlOgiH-vk0APVFhaybs7M7v2HHAuGOAYK_hqgedwX9FC0jrjyJyGSl3lVUO5RndSbBXI_40E5HdXHzon2M1sNtRj0-cMLdJ0iVbdDKXRCOzCttDTQ4puK85gZ5e63owHgYFm-Cel-MxVjeAK0GiRvr2iueAauB5fDv0jLak04A57ba9H0HmJNhVfA0MSlGhlDgqN5klioRcTzzUtQF1dq3_XJf3qPiuDBom28NEYVWzhGmRNK2a-_tCRMqxLqCHDsbkyKtcZzvjR4GirRbLnOY61bMHCUrRFeU7ahlF99uxcNK68C2HST7pRJgdWaSHfBp97u3sMjg94h-NgYpPImYau5weTkISuhfc4Jq4duST0AsfzfH_q-ORo4Z9e1bUdhxA_ckjku8HEifzjL6b9u78?type=png)](https://mermaid.live/edit#pako:eNp1UctuwjAQ_BXLJ5DSNDHkeeBQ-qBVKyGSU5WLG2-TCGKntlOgiH-vk0APVFhaybs7M7v2HHAuGOAYK_hqgedwX9FC0jrjyJyGSl3lVUO5RndSbBXI_40E5HdXHzon2M1sNtRj0-cMLdJ0iVbdDKXRCOzCttDTQ4puK85gZ5e63owHgYFm-Cel-MxVjeAK0GiRvr2iueAauB5fDv0jLak04A57ba9H0HmJNhVfA0MSlGhlDgqN5klioRcTzzUtQF1dq3_XJf3qPiuDBom28NEYVWzhGmRNK2a-_tCRMqxLqCHDsbkyKtcZzvjR4GirRbLnOY61bMHCUrRFeU7ahlF99uxcNK68C2HST7pRJgdWaSHfBp97u3sMjg94h-NgYpPImYau5weTkISuhfc4Jq4duST0AsfzfH_q-ORo4Z9e1bUdhxA_ckjku8HEifzjL6b9u78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What is Web Scraping?\n",
    "---\n",
    "\n",
    "**Definition:** Extracting data from websites programmatically.\n",
    "\n",
    "**Why scrape?**\n",
    "- Converts non-tabular or poorly structured data into usable formats like .csv files, spreadsheets, or database-compatible formats.\n",
    "- Access information not available via APIs.\n",
    "- Automate data collection for analysis.\n",
    "\n",
    "**Use Cases:**\n",
    "- Data Collection: Acquire specific data points from websites.\n",
    "- Data Archiving: Preserve online data for future reference.\n",
    "- Change Tracking: Monitor updates or changes to online information\n",
    "- Examples:\n",
    "  - Data for machine learning.\n",
    "  - Price comparison/Competitive Analysis: Businesses scrape competitor prices to adjust their own.\n",
    "  - Contact Scraping: Collecting personal information for marketing purposes.\n",
    "  - Academic Research/Trend Analysis: Creating datasets for text mining and analysis in scholarly projects.\n",
    "  - Data Journalism: Investigative journalists scrape data for stories, especially when not easily accessible.\n",
    "\n",
    "---\n",
    "## Tools for Web Scraping\n",
    "---\n",
    "\n",
    "**Languages:** Python (preferred), but also PHP or Ruby (i.e., other scripting languages).\n",
    "\n",
    "**Python Libraries:**\n",
    "- **Requests:** https://requests.readthedocs.io/en/latest/  \n",
    "- **Beautiful Soup:** https://beautiful-soup-4.readthedocs.io/\n",
    "- **Scrapy:** https://scrapy.org/\n",
    "- **Selenium:** https://selenium-python.readthedocs.io/ \n",
    "- **Parsel:** https://parsel.readthedocs.io/en/latest/\n",
    "\n",
    "**Other Tools:**\n",
    "- **Browser Developer Tools** for inspection (Shortcut: `F12`)\n",
    "- **Regex** for string patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## A first example\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a local copy of the `index.html`. (Later you can keep it in memory without writing it to the disk.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Specify the URL of the page you want to download\n",
    "url = \"https://www.scrapethissite.com/pages/simple/\"\n",
    "\n",
    "# Make a GET request to fetch the raw HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the content to an HTML file\n",
    "    with open(\"index.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(response.text)\n",
    "    print(\"Page saved as 'page.html'\")\n",
    "else:\n",
    "    print(f\"Failed to download page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the file and get familiar with the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As an alternative you can also use the Developer Tools of your web browser!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# If the HTML is saved locally, you can use:\n",
    "with open(\"index.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "# Initialize an empty dictionary to store the country data\n",
    "countries_dict = {}\n",
    "\n",
    "# Find all div elements with class 'col-md-4 country'\n",
    "countries = soup.find_all('div', class_='col-md-4 country')\n",
    "\n",
    "# Iterate over each country div\n",
    "for country in countries:\n",
    "    # Extract country name\n",
    "    name = country.find('h3', class_='country-name').get_text(strip=True).split('\\n')[-1].strip()\n",
    "    \n",
    "    # Extract capital, population, and area\n",
    "    capital = country.find('span', class_='country-capital').get_text(strip=True)\n",
    "    population = country.find('span', class_='country-population').get_text(strip=True)\n",
    "    area = country.find('span', class_='country-area').get_text(strip=True)\n",
    "\n",
    "    # Store the data in the dictionary\n",
    "    countries_dict[name] = {\n",
    "        'Capital': capital,\n",
    "        'Population': population,\n",
    "        'Area (km^2)': area\n",
    "    }\n",
    "\n",
    "# Print the resulting dictionary\n",
    "for country, data in countries_dict.items():\n",
    "    print(f\"{country}: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "## Don’t break the web: Denial of Service attacks\n",
    "---\n",
    "\n",
    "**Web Scraping Basics:**\n",
    "- Web scraping involves repeatedly querying a website and accessing multiple pages.\n",
    "- Each request to a web server consumes its resources, potentially impacting other users.\n",
    "\n",
    "**Risks of Excessive Requests:**\n",
    "- Sending too many requests in a short time can overload a server.\n",
    "- Overloading may block legitimate users or crash the server.\n",
    "- Hackers exploit this technique intentionally in Denial of Service (DoS) attacks.\n",
    "\n",
    "**Server Protections Against Abuse:**\n",
    "- Servers monitor for excessive requests from a single IP address.\n",
    "- They may block or ban the source to prevent misuse.\n",
    "\n",
    "**Challenges for Scrapers:**\n",
    "- Legitimate web scrapers can inadvertently mimic DoS attacks.\n",
    "- This may result in being banned from the website.\n",
    "\n",
    "**Preventive Measures:**\n",
    "- Insert random delays between requests to the server.\n",
    "- Example: Scrapy includes built-in safeguards like random delays between requests to avoid overloading servers. By default, Scrapy limits the risk of resembling a DoS attack.\n",
    "\n",
    "**Best Practices for Developers:**\n",
    "- Limit the number of pages scraped during development and debugging.\n",
    "- Use Scrapy’s allowed_domains property to restrict scraping to specific domains.\n",
    "- Avoid scraping at full scale until the scraper is thoroughly tested.\n",
    "\n",
    "**In summary:** With proper precautions, the risk of causing trouble is minimal! (Learn to play by the rules)\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Don’t steal: Copyright and fair use\n",
    "--- \n",
    "\n",
    "**Legal Considerations of Web Scraping:**\n",
    "- Scraping may be illegal if a website’s terms and conditions explicitly prohibit copying its content.\n",
    "- Violating such terms could lead to legal trouble.\n",
    "  \n",
    "**General Acceptance:**\n",
    "- Web scraping is often tolerated if it does not disrupt the regular use of a website.\n",
    "- It is comparable to using a browser to access publicly available web content.\n",
    "\n",
    "**Public vs. Protected Data:**\n",
    "- Scraping publicly available data (not behind authentication or paywalls) is generally acceptable.\n",
    "- Problems arise when scraped data is redistributed or shared inappropriately.\n",
    "\n",
    "**Copyright Concerns:**\n",
    "- Republishing scraped content, especially without permission, can constitute copyright infringement.\n",
    "- Simply posting content from one site onto another as one’s own is illegal.\n",
    "\n",
    "**Fair Use Exceptions:**\n",
    "- Using scraped data in aggregate or derivative formats may qualify as “fair use.”\n",
    "- Avoid passing off scraped data as original, copying it verbatim, or monetizing it without permission.\n",
    "\n",
    "**Legal Variability:**\n",
    "- Copyright and data privacy laws vary by country; check the laws applicable to your location.\n",
    "- Example: In Australia, scraping and storing personal information (e.g., names, phone numbers, email addresses) can be illegal, even if publicly available.\n",
    "\n",
    "**Personal vs. Large-Scale Use:**\n",
    "- For personal use, following general scraping guidelines is usually sufficient.\n",
    "- For large-scale research or commercial projects, seek legal advice beforehand.\n",
    "\n",
    "**University Resources:**\n",
    "- Universities often have a copyright office to assist with legal aspects of data scraping projects.\n",
    "- The university library is a good starting point for guidance on copyright issues.\n",
    "\n",
    "**Best Practices:** Scrape responsibly, respect terms of use, and ensure compliance with copyright laws.\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## robots.txt\n",
    "---\n",
    "\n",
    "**What is robots.txt?**\n",
    "- A file placed at the root of a website (example.com/robots.txt) to communicate with web crawlers and bots.\n",
    "- It specifies which parts of the website can or cannot be accessed by automated programs.\n",
    "\n",
    "**Purpose of robots.txt:**\n",
    "- Helps manage server load by restricting unnecessary crawling.\n",
    "- Protects sensitive or non-public sections of a site from being indexed.\n",
    "- Provides guidelines for ethical web scraping and crawling.\n",
    "\n",
    "**How robots.txt Works:**\n",
    "- Uses directives to instruct bots (e.g., User-agent: * applies to all bots).\n",
    "\n",
    "**Common directives include:**\n",
    "- `Disallow`: specifies paths that bots should not crawl.\n",
    "- `Allow`: permits access to specific paths.\n",
    "- `Sitemap`: points bots to the website’s sitemap for better indexing.\n",
    "\n",
    "**Limitations of robots.txt:**\n",
    "- It is advisory, not enforceable; malicious or non-compliant bots can ignore it.\n",
    "- Does not provide security; sensitive content should be protected by other means (e.g., authentication).\n",
    "\n",
    "**Best Practices for Web Scraping:**\n",
    "- Always check a website’s robots.txt file before scraping.\n",
    "- Respect the rules specified in the file to avoid violating ethical or legal standards.\n",
    "- Use tools or libraries (e.g., Python’s robotsparser) to parse and adhere to robots.txt. https://docs.python.org/3/library/urllib.robotparser.html\n",
    "\n",
    "**Locating robots.txt:**\n",
    "- Visit the URL https://[website-domain]/robots.txt to view its contents.\n",
    "- Note that not all websites have a robots.txt file.\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Web scraping code of conduct\n",
    "---\n",
    "\n",
    "If you adhere to the following simple rules, you will probably be fine.\n",
    "\n",
    "1. **Ask nicely.** If your project requires data from a particular organisation, for example, you can try asking them directly if they could provide you what you are looking for. With some luck, they will have the primary data that they used on their website in a structured format, saving you the trouble.\n",
    "2. **Don’t download copies of documents that are clearly not public.** For example, academic journal publishers often have very strict rules about what you can and what you cannot do with their databases. Mass downloading article PDFs is probably prohibited and can put you (or at the very least your friendly university librarian) in trouble. If your project requires local copies of documents (e.g. for text mining projects), special agreements can be reached with the publisher. The library is a good place to start investigating something like that.\n",
    "3. **Check your local legislation.** For example, certain countries have laws protecting personal information such as email addresses and phone numbers. Scraping such information, even from publicly avaialable web sites, can be illegal (e.g. in Australia).\n",
    "4. **Don’t share downloaded content illegally.** Scraping for personal purposes is usually OK, even if it is copyrighted information, as it could fall under the fair use provision of the intellectual property legislation. However, sharing data for which you don’t hold the right to share is illegal.\n",
    "Share what you can. If the data you scraped is in the public domain or you got permission to share it, then put it out there for other people to reuse it (e.g. on datahub.io). If you wrote a web scraper to access it, share its code (e.g. on GitHub) so that others can benefit from it.\n",
    "5. **Don’t break the Internet.** Not all web sites are designed to withstand thousands of requests per second. If you are writing a recursive scraper (i.e. that follows hyperlinks), test it on a smaller dataset first to make sure it does what it is supposed to do. Adjust the settings of your scraper to allow for a delay between requests. By default, Scrapy uses conservative settings that should minimize this risk.\n",
    "6. **Publish your own data in a reusable way.** Don’t force others to write their own scrapers to get at your data. Use open and software-agnostic formats (e.g. JSON, XML), provide metadata (data about your data: where it came from, what it represents, how to use it, etc.) and make sure it can be indexed by search engines so that people can find it.\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Video recommendations \n",
    "---\n",
    "\n",
    "[David Kriesel - SpiegelMining – Reverse Engineering von Spiegel-Online @ 33c3](https://media.ccc.de/v/33c3-7912-spiegelmining_reverse_engineering_von_spiegel-online)\n",
    "\n",
    "<iframe width=\"512\" height=\"288\" src=\"https://media.ccc.de/v/33c3-7912-spiegelmining_reverse_engineering_von_spiegel-online/oembed\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\n",
    "[David Kriesel - BahnMining - Pünktlichkeit ist eine Zier @ 36C3](https://media.ccc.de/v/36c3-10652-bahnmining_-_punktlichkeit_ist_eine_zier)\n",
    "\n",
    "<iframe width=\"512\" height=\"288\" src=\"https://media.ccc.de/v/36c3-10652-bahnmining_-_punktlichkeit_ist_eine_zier/oembed\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "## Further reading\n",
    "---\n",
    "\n",
    "https://en.wikipedia.org/wiki/Data_journalism  \n",
    "https://en.wikipedia.org/wiki/Web_scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key points\n",
    "---\n",
    "\n",
    "- Web scraping is, in general, legal and won’t get you into trouble.\n",
    "\n",
    "- There are a few things to be careful about, notably don’t overwhelm a web server and don’t steal content.\n",
    "\n",
    "- Be nice. In doubt, ask.\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lab assignment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping and evaluating NHL team stats since 1990\n",
    "\n",
    "Open this site to browse through a database of NHL team stats since 1990:\n",
    "\n",
    "https://www.scrapethissite.com/pages/forms/\n",
    "\n",
    "Luckily, the data is already provided in tabular format. However, not all of the data is shown in a single page. Instead, you will have to paginate through the database. In the following, your task is to scrape all of the data and find a way to automatically scrape all of the pages and fetch the complete database. Then write everything into a CSV file. The columns names should correspond to those in the HTML table. Afterwards you will load the CSV file with pandas and give answers to the following two questions:\n",
    "\n",
    "- How made the most \"wins\" in 1990, 2000, and 2010?\n",
    "- How many teams participated in 1991, 2001, and 2011?\n",
    "\n",
    "**In summary:**\n",
    "1. Scrape the data (find a way to obtain all data from a pages programmatically)\n",
    "2. Store the entire data as CSV file\n",
    "3. Load it with pandas\n",
    "4. Give answers to the questions above by making use of the DataFrame methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the example above, we recommend to use **requests** and **beautifulsoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Losses</th>\n",
       "      <th>OT Losses</th>\n",
       "      <th>Win %</th>\n",
       "      <th>Goals For (GF)</th>\n",
       "      <th>Goals Against (GA)</th>\n",
       "      <th>+ / -</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boston Bruins</td>\n",
       "      <td>1990</td>\n",
       "      <td>44</td>\n",
       "      <td>24</td>\n",
       "      <td></td>\n",
       "      <td>0.55</td>\n",
       "      <td>299</td>\n",
       "      <td>264</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buffalo Sabres</td>\n",
       "      <td>1990</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td></td>\n",
       "      <td>0.388</td>\n",
       "      <td>292</td>\n",
       "      <td>278</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Calgary Flames</td>\n",
       "      <td>1990</td>\n",
       "      <td>46</td>\n",
       "      <td>26</td>\n",
       "      <td></td>\n",
       "      <td>0.575</td>\n",
       "      <td>344</td>\n",
       "      <td>263</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicago Blackhawks</td>\n",
       "      <td>1990</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td></td>\n",
       "      <td>0.613</td>\n",
       "      <td>284</td>\n",
       "      <td>211</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Detroit Red Wings</td>\n",
       "      <td>1990</td>\n",
       "      <td>34</td>\n",
       "      <td>38</td>\n",
       "      <td></td>\n",
       "      <td>0.425</td>\n",
       "      <td>273</td>\n",
       "      <td>298</td>\n",
       "      <td>-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Edmonton Oilers</td>\n",
       "      <td>1990</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td></td>\n",
       "      <td>0.463</td>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hartford Whalers</td>\n",
       "      <td>1990</td>\n",
       "      <td>31</td>\n",
       "      <td>38</td>\n",
       "      <td></td>\n",
       "      <td>0.388</td>\n",
       "      <td>238</td>\n",
       "      <td>276</td>\n",
       "      <td>-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Los Angeles Kings</td>\n",
       "      <td>1990</td>\n",
       "      <td>46</td>\n",
       "      <td>24</td>\n",
       "      <td></td>\n",
       "      <td>0.575</td>\n",
       "      <td>340</td>\n",
       "      <td>254</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Minnesota North Stars</td>\n",
       "      <td>1990</td>\n",
       "      <td>27</td>\n",
       "      <td>39</td>\n",
       "      <td></td>\n",
       "      <td>0.338</td>\n",
       "      <td>256</td>\n",
       "      <td>266</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Montreal Canadiens</td>\n",
       "      <td>1990</td>\n",
       "      <td>39</td>\n",
       "      <td>30</td>\n",
       "      <td></td>\n",
       "      <td>0.487</td>\n",
       "      <td>273</td>\n",
       "      <td>249</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>New Jersey Devils</td>\n",
       "      <td>1990</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td></td>\n",
       "      <td>0.4</td>\n",
       "      <td>272</td>\n",
       "      <td>264</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>New York Islanders</td>\n",
       "      <td>1990</td>\n",
       "      <td>25</td>\n",
       "      <td>45</td>\n",
       "      <td></td>\n",
       "      <td>0.312</td>\n",
       "      <td>223</td>\n",
       "      <td>290</td>\n",
       "      <td>-67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>New York Rangers</td>\n",
       "      <td>1990</td>\n",
       "      <td>36</td>\n",
       "      <td>31</td>\n",
       "      <td></td>\n",
       "      <td>0.45</td>\n",
       "      <td>297</td>\n",
       "      <td>265</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Philadelphia Flyers</td>\n",
       "      <td>1990</td>\n",
       "      <td>33</td>\n",
       "      <td>37</td>\n",
       "      <td></td>\n",
       "      <td>0.412</td>\n",
       "      <td>252</td>\n",
       "      <td>267</td>\n",
       "      <td>-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Pittsburgh Penguins</td>\n",
       "      <td>1990</td>\n",
       "      <td>41</td>\n",
       "      <td>33</td>\n",
       "      <td></td>\n",
       "      <td>0.512</td>\n",
       "      <td>342</td>\n",
       "      <td>305</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Quebec Nordiques</td>\n",
       "      <td>1990</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td></td>\n",
       "      <td>0.2</td>\n",
       "      <td>236</td>\n",
       "      <td>354</td>\n",
       "      <td>-118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>St. Louis Blues</td>\n",
       "      <td>1990</td>\n",
       "      <td>47</td>\n",
       "      <td>22</td>\n",
       "      <td></td>\n",
       "      <td>0.588</td>\n",
       "      <td>310</td>\n",
       "      <td>250</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Toronto Maple Leafs</td>\n",
       "      <td>1990</td>\n",
       "      <td>23</td>\n",
       "      <td>46</td>\n",
       "      <td></td>\n",
       "      <td>0.287</td>\n",
       "      <td>241</td>\n",
       "      <td>318</td>\n",
       "      <td>-77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Vancouver Canucks</td>\n",
       "      <td>1990</td>\n",
       "      <td>28</td>\n",
       "      <td>43</td>\n",
       "      <td></td>\n",
       "      <td>0.35</td>\n",
       "      <td>243</td>\n",
       "      <td>315</td>\n",
       "      <td>-72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Washington Capitals</td>\n",
       "      <td>1990</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td></td>\n",
       "      <td>0.463</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Winnipeg Jets</td>\n",
       "      <td>1990</td>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td></td>\n",
       "      <td>0.325</td>\n",
       "      <td>260</td>\n",
       "      <td>288</td>\n",
       "      <td>-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Boston Bruins</td>\n",
       "      <td>1991</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td></td>\n",
       "      <td>0.45</td>\n",
       "      <td>270</td>\n",
       "      <td>275</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Buffalo Sabres</td>\n",
       "      <td>1991</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td></td>\n",
       "      <td>0.388</td>\n",
       "      <td>289</td>\n",
       "      <td>299</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Calgary Flames</td>\n",
       "      <td>1991</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td></td>\n",
       "      <td>0.388</td>\n",
       "      <td>296</td>\n",
       "      <td>305</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chicago Blackhawks</td>\n",
       "      <td>1991</td>\n",
       "      <td>36</td>\n",
       "      <td>29</td>\n",
       "      <td></td>\n",
       "      <td>0.45</td>\n",
       "      <td>257</td>\n",
       "      <td>236</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Team Name  Year Wins Losses OT Losses  Win % Goals For (GF)  \\\n",
       "0           Boston Bruins  1990   44     24             0.55            299   \n",
       "1          Buffalo Sabres  1990   31     30            0.388            292   \n",
       "2          Calgary Flames  1990   46     26            0.575            344   \n",
       "3      Chicago Blackhawks  1990   49     23            0.613            284   \n",
       "4       Detroit Red Wings  1990   34     38            0.425            273   \n",
       "5         Edmonton Oilers  1990   37     37            0.463            272   \n",
       "6        Hartford Whalers  1990   31     38            0.388            238   \n",
       "7       Los Angeles Kings  1990   46     24            0.575            340   \n",
       "8   Minnesota North Stars  1990   27     39            0.338            256   \n",
       "9      Montreal Canadiens  1990   39     30            0.487            273   \n",
       "10      New Jersey Devils  1990   32     33              0.4            272   \n",
       "11     New York Islanders  1990   25     45            0.312            223   \n",
       "12       New York Rangers  1990   36     31             0.45            297   \n",
       "13    Philadelphia Flyers  1990   33     37            0.412            252   \n",
       "14    Pittsburgh Penguins  1990   41     33            0.512            342   \n",
       "15       Quebec Nordiques  1990   16     50              0.2            236   \n",
       "16        St. Louis Blues  1990   47     22            0.588            310   \n",
       "17    Toronto Maple Leafs  1990   23     46            0.287            241   \n",
       "18      Vancouver Canucks  1990   28     43             0.35            243   \n",
       "19    Washington Capitals  1990   37     36            0.463            258   \n",
       "20          Winnipeg Jets  1990   26     43            0.325            260   \n",
       "21          Boston Bruins  1991   36     32             0.45            270   \n",
       "22         Buffalo Sabres  1991   31     37            0.388            289   \n",
       "23         Calgary Flames  1991   31     37            0.388            296   \n",
       "24     Chicago Blackhawks  1991   36     29             0.45            257   \n",
       "\n",
       "   Goals Against (GA) + / -  \n",
       "0                 264    35  \n",
       "1                 278    14  \n",
       "2                 263    81  \n",
       "3                 211    73  \n",
       "4                 298   -25  \n",
       "5                 272     0  \n",
       "6                 276   -38  \n",
       "7                 254    86  \n",
       "8                 266   -10  \n",
       "9                 249    24  \n",
       "10                264     8  \n",
       "11                290   -67  \n",
       "12                265    32  \n",
       "13                267   -15  \n",
       "14                305    37  \n",
       "15                354  -118  \n",
       "16                250    60  \n",
       "17                318   -77  \n",
       "18                315   -72  \n",
       "19                258     0  \n",
       "20                288   -28  \n",
       "21                275    -5  \n",
       "22                299   -10  \n",
       "23                305    -9  \n",
       "24                236    21  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as rs \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "\n",
    "page = rs.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.text,\"html\")\n",
    "\n",
    "table = soup.find_all(\"tbody\")\n",
    "\n",
    "world_titles = soup.find_all(\"th\")\n",
    "world_table_titles = [ title.text.strip() for title in world_titles]\n",
    "\n",
    "#print(world_table_titles)\n",
    "\n",
    "df = pd.DataFrame(columns = world_table_titles)\n",
    "\n",
    "column_data = soup.find_all(\"tr\")\n",
    "for row in column_data[1:]:\n",
    "    row_data = row.find_all(\"td\")\n",
    "    individual_row_data = [ data.text.strip() for data in row_data]\n",
    "\n",
    "    length = len(df)\n",
    "    df.loc[length] = individual_row_data\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rs\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Basis-URL der Webseite\n",
    "base_url = \"https://www.scrapethissite.com/pages/forms/?page_num={}&per_page=25\"\n",
    "\n",
    "# DataFrame initialisieren\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Gesamtzahl der Seiten (24 Seiten)\n",
    "total_pages = 24\n",
    "\n",
    "# Durch alle Seiten iterieren\n",
    "for page in range(1, total_pages + 1):\n",
    "    url = base_url.format(page)\n",
    "    page = rs.get(url)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    # Tabellenheader extrahieren (nur für die erste Seite notwendig)\n",
    "    if df.empty:\n",
    "        world_titles = soup.find_all(\"th\")\n",
    "        world_table_titles = [title.text.strip() for title in world_titles]\n",
    "        df = pd.DataFrame(columns=world_table_titles)\n",
    "    \n",
    "    # Tabelleninhalte extrahieren\n",
    "    column_data = soup.find_all(\"tr\")\n",
    "    for row in column_data[1:]:\n",
    "        row_data = row.find_all(\"td\")\n",
    "        individual_row_data = [data.text.strip() for data in row_data]\n",
    "\n",
    "        # Nur Zeilen hinzufügen, die Daten enthalten\n",
    "        if individual_row_data:\n",
    "            df.loc[len(df)] = individual_row_data\n",
    "\n",
    "df\n",
    "\n",
    "df.to_csv(\"/Uni/DIS08/Aufgabe8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Team mit den meisten Siegen in 1990 war: Chicago Blackhawks mit 49 Siegen.\n",
      "\n",
      "Das Team mit den meisten Siegen in 2000 war: Colorado Avalanche mit 52 Siegen.\n",
      "\n",
      "Das Team mit den meisten Siegen in 2010 war: Vancouver Canucks mit 54 Siegen.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Anzahl der Teams, die in 1991 teilgenommen haben: 22.\n",
      "\n",
      "Anzahl der Teams, die in 2001 teilgenommen haben: 30.\n",
      "\n",
      "Anzahl der Teams, die in 2011 teilgenommen haben: 30.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Uni/DIS08/Aufgabe8.csv\")\n",
    "\n",
    "# Frage 1: Wer hat die meisten \"Wins\" in 1990, 2000 und 2010?\n",
    "for year in [1990, 2000, 2010]:\n",
    "    filtered_data = df[df['Year'] == year]\n",
    "    max_wins = filtered_data['Wins'].max()\n",
    "    team = filtered_data[filtered_data['Wins'] == max_wins]['Team Name'].iloc[0]\n",
    "    print(f\"Das Team mit den meisten Siegen in {year} war: {team} mit {max_wins} Siegen.\")\n",
    "    print()\n",
    "\n",
    "print(80*\"-\")\n",
    "\n",
    "# Frage 2: Wie viele Teams haben in 1991, 2001 und 2011 teilgenommen?\n",
    "for year in [1991, 2001, 2011]:\n",
    "    team_count = df[df['Year'] == year]['Team Name'].nunique()\n",
    "    print(f\"Anzahl der Teams, die in {year} teilgenommen haben: {team_count}.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have completed the tasks, please commit this notebook with the solution to your GitHub repository in the directory `assignments/08/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
